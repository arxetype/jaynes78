\section{Present Features}
\label{sec:B}
\subsection{The Formalism}
Let us set down, for reference, a bit of the basic Maximum Entropy formalism for the finite discrete case, putting off generalizations until they are needed.
There are $n$ different possibilities, which would be distinguished adequately by a single index ($i=1,2,\ldots,n$).
Nevertheless we find it helpful, both for notation and for the applications we have in mind, to introduce in addition a real variable $x$, which can take on the discrete values ($x_1, 1 \leq i \leq n$), defined in any way and not necessarily all distinct.
If we have certain information $I$ about $x$, the problem is to represent this by a probability distribution $\{p_i\}$ which has maximum entropy while agreeing with $I$.

Clearly, such a problem cannot be well-posed for arbitrary information; I must be such that, given any proposed distribution $\{p_i\}$, we can determine unambiguously whether $I$ does or does not agree with $\{p_i\}$.
Such information will be called testable.
For example, consider:

\vspace{0.5\baselineskip}
	$I_1 \equiv$ \emph{``It is certain that $\tanh x < 0.7$.''}

	$I_2 \equiv$ \emph{``There is at least a $90$\% probability that $\tanh x < 0.7$.''}

	$I_3 \equiv$ \emph{``The mean value of $\tanh x$ is $0.675$.''}

	$I_4 \equiv$ \emph{``The mean value of $\tanh x$ is probably less than $0.7$.''}

	$I_5 \equiv$ \emph{``There is some reason to believe that $\tanh x = 0.675$.''}

\vspace{0.5\baselineskip}
Statements $I_1, I_2, I_3$ are testable, and may be used as constraints in maximizing the entropy.
$I_4$ and $I_5$, although clearly relevant to inference about $x$, are too vague to be testable, and we have at present no formal principle by which such information can be used in a mathematical theory.
However, the fact that our intuitive common sense does make use of non-testable information suggests that new principles for this, as yet undiscovered, must exist.

Since $n$ is finite, the entropy has an absolute maximum value $\log n$, and any constraint can only lower this.
If we think of the $\{p_i\}$ as cartesian coordinates of a point $P$ in an $n$-dimensional space, $P$ is constrained by $p_i > 0$, $\sum p_i=1$ to lie on a domain $D$ which is a ``triangular'' segment of an $(n-1)$-dimensional hyperplane.
On $D$ the entropy varies continuously, taking on all values in $0 \leq H \leq \log n$ and reaching its absolute maximum at the center.
Any testable information will restrict $P$ to some sub-region $D'$ of $D$, and clearly the entropy has some least upper bound $H \leq \log n$ on $D'$.
So the maximum entropy problem must have a solution if $D'$ is a closed set.

There may be more than one solution: for example, the information $I_6 \equiv$ ``The entropy of the distribution $\{p_i\}$ is not greater than $\log(n-1)$'' is clearly testable, and if $n>2$ it yields an infinite number of solutions.
Furthermore, strictly speaking, if $D'$ is an open set there may not be any solution, the upper bound being approached but not actually reached on $D'$.
Such a case is generated by $I_7 \equiv$ ``$p_1^2 + p_2^2 < n^{-2}$.''
However, since we are concerned with physical problems where the distinction between open and closed sets cannot matter, we would accept a point on the closure of $D'$ (in this example, on its boundary) as a valid solution, although corresponding strictly only to $I_8 \equiv$ ``$p_1^2 + p_2^2 \leq n^{-2}$.''

But these considerations are mathematical niceties that one has to mention only because he will be criticized if he does not.
In the real applications that matter, we have not yet found a case which does not have a unique solution.

In principle, every different kind of testable information will generate a different kind of mathematical problem.
But there is one important class of problems for which the general solution was given once and for all, by Gibbs.
If the constraints consist of specifying mean values of certain functions $\{f_1(x) ,f_2(x),\ldots, f_m(x)\}$:
\begin{equation}
	\label{B1}
	\sum_{i=1}^{n} p_i f_k(x_i) = F_k, \qquad 1 \leq k \leq m
\end{equation}
where $\{F_k\}$ are numbers given in the statement of the problem, then if $m < n$, entropy maximization is a standard variational problem solvable by stationarity using the Lagrange multiplier technique.
It has the formal solution:
\begin{equation}
	\label{B2}
	P_i = \frac{1}{Z(\lambda_1, \ldots \lambda_m)} \exp\left[-\lambda_1f_1(x_i) - \cdots - \lambda_mf_m(x_i)\right]
\end{equation}
where
\begin{equation}
	\label{B3}
	Z(\lambda_1, \ldots \lambda_m) = \sum_{i=1}{n} \exp\left[-\lambda_1f_1(x_i) - \cdots - \lambda_mf_m(x_i)\right]
\end{equation}
is the partition function and $\{\lambda_k\}$ are the Lagrange multipliers, which are chosen so as to satisfy the constraints \eqref{B1}.
This is the case if
\begin{equation}
	\label{B4}
	F_k = -\pdv{}{\lambda_k} \log Z, \qquad 1\leq k\leq m
\end{equation}
a set of $m$ simultaneous equations for $m$ unknowns.
The value of the entropy maximum then attained is, as noted in my reminiscences, a function only of the given data:
\begin{equation}
	\label{B5}
	S(F_1, \ldots, F_m) = \log Z + \sum_{k} \lambda_k F_k
\end{equation}
and if this function were known, the explicit solution of \eqref{B4} would be
\begin{equation}
	\label{B6}
	\lambda_k = \pdv{S}{F_k}, \qquad 1\leq k\leq m
\end{equation}
Given this distribution, the best prediction we can make (in the sense of minimizing the expected square of the error) of any quantity $q(x)$, is then
\begin{equation*}
	\mean{q(x)} = \sum_{i=1}^{n} p_i q(x_i)
\end{equation*}
and numerous covariance and reciprocity rules are contained in the identity
\begin{equation}
	\label{B7}
	\mean{qf_k} - \mean{q} \mean{f_k} = - \pdv{\mean{q}}{\lambda_k}
\end{equation}
[note the special cases $q(x) = f_j(x)$, and $j=k$].
The functions $f_k(x)$ may contain also some parameters $\alpha_j$:
\begin{equation*}
	f_k = f_k(x; \alpha_1, \ldots, \alpha_s)
\end{equation*}
(which in physical applications might have the meaning of volume, magnetic field intensity, angular velocity, etc.);
and we have an important variational property: if we make an arbitrary small change in all the data of the problem $\{\ddel F_k, \ddel \alpha_r\}$, we may compare two slightly different maximum-entropy solutions.
The difference in their entropies is found, after some calculation, to be
\begin{equation}
	\label{B8}
	\ddel S = \sum_{k} \lambda_k \cdot \ddel Q_k
\end{equation}
where
\begin{equation}
	\label{B9}
	\ddel Q_k \equiv \ddel \mean{f_k} - \mean{\ddel f_k}
\end{equation}
The meaning of this identity has a familiar ring: there is no such function as $Q_k(F1 \ldots F_m; \alpha_1 \ldots \alpha_s)$ because $\ddel Q_k$ is not an exact differential.
However, the Lagrange multiplier $\lambda_k$ is an integrating factor such that $\sum \lambda_k \ddel Q_k$, is the exact differential of a ``state function'' $S(F_1 \ldots F_m; \alpha_1 \ldots \alpha_s)$.

I believe that Clausius would recognize here an interesting echo of his work, although we have only stated some general rules for plausible reasoning, making no necessary reference to physics.
This is enough of the bare skeleton of the formalism to serve as the basis for some examples and discussion.

\subsection{Brandeis Dice Problem}
First, we illustrate the formalism by working out the numerical solution to a problem which was used in the Introduction to my 1962 Brandeis lectures merely as a qualitative illustration of the ideas, but has since become a \emph{cause célébre} as some papers have been written attacking the Principle of Maximum Entropy on the grounds of this very example.
So a close look at it will take us straight to the heart of some of the most common misconceptions and, I hope, give us some appreciation of what the Principle of Maximum Entropy does and does not (indeed, should not) accomplish for us.

When a die is tossed, the number of spots up can have any value $i$ in $1\leq i\leq 6$.
Suppose a die has been tossed $N$ times and we are told only that the average number of spots up was not $3.5$ as we might expect from an ``honest'' die but $4.5$.
Given this information, and nothing else, what probability should we assign to $i$ spots on the next toss?
The Brandeis Lectures started with a qualitative graphical discussion of this problem, which showed (or so I thought) how ordinary common sense forces us to a result with the qualitative properties of the maximum entropy solution.

Let us see what solution the Principle of Maximum Entropy gives for this problem, if we interpret the data as imposing the mean value constraint
\begin{equation}
	\label{B10}
	\sum_{i=1}^{6} i\cdot p_i = 4.5
\end{equation}
The partition function is
\begin{equation}
	\label{B11}
	Z(\lambda) = \sum_{i} e^{-\lambda i} = x(1-x)^{-1} (1-x^6)
\end{equation}
where $x = e^\lambda$.
The constraint \eqref{B10} then becomes
\begin{equation*}
	-\pdv{}{\lambda} \log Z = \frac{1 - 7x^6 + 6x^6}{(1-x)(1-x^6)} = 4.5
\end{equation*}
or
\begin{equation}
	\label{B12}
	3x^7 - 5x^6 + 9x - 7 = 0
\end{equation}
By computer, the desired root of this is $x = 1.44925$, which yields $\lambda = -0.37105$, $Z = 26.66365$, $\log Z = 3.28330$.
The maximum-entropy probabilities are $p_i = Z^{-1}x^i$, or
\begin{equation}
	\label{B13}
	\begin{split}
		\{p_1\ldots p_6\}
		&= \{0.05435,\, 0.07877,\, 0.11416,\,\\
		&\hphantom{= \{\ } 0.16545,\, 0.23977,\, 0.34749\}
	\end{split}
\end{equation}
Fron \eqref{B5}, the entropy of this distribution is
\begin{equation}
	\label{B14}
	S = 1.61358\ \mathrm{natural\ units}
\end{equation}
as compared to the maximum of $\log_e 6 = 1.79176$, corresponding to no constraints and a uniform distribution.

Now, what does this result mean?
In the first place, it is a distribution $\{p_r, 1 \leq r \leq 6\}$ on a space of only six points; the sample space $S$ of a single trial.
Therefore, our result as it stands is only a mean of describing a state of knowledge about the outcome of a single trial.
It represents a state of knowledge in which one has only (1) the enumeration of the six possibilities; and (2) the mean value constraint \eqref{B10}; and \emph{no other information}.
The distribution is ``maximally noncommittal'' with respect to all other matters; it is as uniform (by the criterion of the Shannon information measure) as it can get without violating the given constraint.

Any probability distribution over some sample space $S$ enables us to make statements about (i.e., assign probabilities to) propositions or events defined within that space.
It does not--and by its very nature cannot--make statements about any event lying outside that space.
Therefore, our maximum-entropy distribution does not, and cannot, make any statement about \emph{frequencies}.

Anything one says about a frequency in $n$ tosses is a statement about an event in the $n$-fold extension space $S^n = S \otimes S \otimes \cdots \otimes S$ of $n$ tosses, containing $6^n$ points (and of course, in any higher space which has $S^n$ as a subspace).

It may be common practice to jump to the conclusion that a probability in one space is the same as a frequency in a different space; and indeed, the level of many expositions is such that the distinction is not recognized at all.
But the first thing one has to learn about using the Principle of Maximum Entropy in real problems is that the mathematical rules of probability theory must be obeyed strictly; all conceptual sloppiness of this sort must be recognized and expunged.

There is, indeed, a connection between a \emph{probability} $p_i$ in space $S$ and a \emph{frequency} $g_i$ in $S^n$;
but we are justified in using only those connections which are deducible from the mathematical rules of probability theory.
As we shall see in connection with fluctuation theory, some common attempts to identify probability and frequency actually stand in conflict with the rules of probability theory.


\subsection{Probability and Frequency}
To derive the simplest and most general connection, the sample space $S^n$ of $n$ trials may be labeled by $\{r_1,r_2,\ldots,r_n\}$, where $1\leq r_k \leq 6$, and $r_k$ is the number of spots up on the $k$'th toss.
The most general probability assignment on $S^n$ is a set of non-negative real numbers $P(r_1\ldots r_n)$ such that
\begin{equation}
	\label{B15}
	\sum_{r_1=1}^{6} \cdots \sum_{r_n=1}^{6} P(r_1\ldots r_n) = 1
\end{equation}
In any given sequence $\{r_1\ldots r_n\}$ of results, the frequency with which $i$ spots occurs is
\begin{equation}
	\label{B16}
	g_i(r_1\ldots r_n) = n^{-1} \sum_{k=1}^{n} \delta(r_k, i)
\end{equation}
This can take on $(n+1)$ discrete values, and its expectation is
\begin{equation}
\label{B17}
\begin{split}
	\mean{g_i}
	&= \frac{1}{n} \sum_{k=1}^{n}  \sum_{r_1=1}^{6} \cdots \sum_{r_n=1}^{6} P(r_1\ldots r_n) \ddel(r_k, i)\\
	&= \frac{1}{n} (p_1(i) + p_2(i) + \cdots + p_n(i))
\end{split}
\end{equation}
where $p_k(i)$ is the probability of getting $i$ spots on the $k$'th toss, regardless of what happens in other tosses.
The expected frequency of an event is always equal to its \emph{average} probability over the different trials.

Many experiments fall into the category of \emph{exchangeable sequences}; i.e., it is clear that the underlying ``mechanism'' of the experiment, although unknown, is not changing from one trial to another.
The probability of any particular sequence of results $\{r_1,\ldots,r_n\}$ should then depend only on how many times a particular outcome $r=i$ happened; and not on which particular trials.
Then the probability distribution $p\{r_k\}$ is invariant under permutations of the labels $k$.
In this case, the probability of $i$ spots is the same at each trial: $p_1(i) = p_2(i) = \cdots = p_n(i) = p_i$ and \eqref{B17} becomes
\begin{equation}
	\label{B18}
	\mean{g_i} = p_i
\end{equation}
In an exchangeable sequence, the probability of an event at one trial is not the same as its frequency in many trials; but it is numerically equal to the \emph{expectation} of that frequency; and this connection holds whatever correlations may exist between different trials.

The probability is therefore the ``best'' estimate of the frequency, in the sense that it minimizes the expected square of the error.
But the result \eqref{B18} tells us nothing whatsoever about whether this is a \emph{reliable} estimate; and indeed nothing in the space $S$ of a single trial can tell us anything about the reliability of \eqref{B18}.

To investigate this, note that by a similar calculation, the expected product of two frequencies is
\begin{equation}
	\label{B19}
	\mean{g_ig_j} = n^{-2} \sum_{k, m = 1}^{n} p_k(i) p(j, m\given i, k)
\end{equation}
where $p(j, m\given i, k)$ is the conditional probability that the $m$'th trial gives the result $j$, given that the $k$'th trial had the outcome $i$.
Of course, if $m=k$ we have simply $p(jk\given ik) = \delta_{ij}$.

In an exchangeable sequence $p(jm \given ik)$ is independent of $m,k$ for $m \neq k$; and so $p_k(i) p(jm\given ik) = \delta_{ij}$, the probability of getting the outcomes $i$, $j$ respectively at any two different tosses.
The covariance of $g_i$, $g_j$ then reduces to
\begin{equation}
	\label{B20}
	\mean{g_i g_j} - \mean{g_i} \mean{g_j} = (p_{ij} - p_i p_j) + \frac{1}{n} (\delta_{ij}p_i - p_{ij})
\end{equation}
If the probabilities are not independent, $p_{ij} \neq p_i p_j$, this does not go to zero for large $n$.

Let us examine the case $i=j$ more closely.
Writing $p_{ii} = \alpha_i p_i$, $\alpha_i$ is the conditional probability that, having obtained the result $i$ on one toss, we shall get it at some other specified toss.
The variance of $g_i$ is, from \eqref{B20}, dropping the index $i$,
\begin{equation}
	\label{B21}
	\mean{g^2} - \mean{g}^2 = p(\alpha - p) + \frac{1}{n} p(1 - \alpha)
\end{equation}
Two extreme cases of inter-trial correlations are contained in \eqref{B21}.
For complete independence, $\alpha = p$, the variance reduces to $n^{-1} p (1-p)$, just the result of the de Moivre-Laplace limit theorem \eqref{A4}.
But as cautioned before, in any other case the variance does not tend to zero at all; there is no ``law of large numbers.''
For complete dependence, $\alpha = 1$ (i.e., having seen the result of one toss, the die is certain to give the same result at all others), \eqref{B21} reduces to $p(1-p)$ which again makes excellent sense; in this case our uncertainty about the frequency in any number of tosses must be just our uncertainty about the first toss.

Note that the variance \eqref{B21} becomes zero for a slight negative correlation:
\begin{equation}
	\label{B22}
	\alpha = p - \frac{1-p}{n-1}
\end{equation}
Due to the permutation invariance of $P(r_i\ldots r_n)$ it is not possible to have a negative correlation stronger than this; as $n\to \infty$ it is not possible to have any negative correlation in an exchangeable sequence.
This corresponds to the famous de Finetti (1937) representation theorem; in the literature of pure mathematics it is called the Hausdorff moment problen.
An almost unbelievably simple proof has just been found by Heath and Sudderth (\cite{heath}{1976}).

To summarize: given any probability assignment $P(r_i\ldots r_n)$ on the space $S^n$, we can determine the probability distribution $W_i(t)$ for the frequency $g_i$ to take on any of its possible values $g_i = (t/n), O\leq t\leq n$.
The (mean) + (standard deviation) over this distribution then provide a reasonable statement of our ``best'' estimate of $g_i$ and its accuracy. In the case of an exchangeable sequence, this estimate is
\begin{equation}
	\label{B23}
	(g_i)_{\mathrm{est}} = p_i \pm \sqrt{p_i (1 - p_i)} \left(R_i + \frac{1-R_i}{n}\right)^{1/2}
\end{equation}
where $R_i = (\alpha_i-p_i)/(1-p_i)$ is a measure of the inter-trial correlation, ranging from $R=0$ for complete independence to $R=1$ for complete dependence.

Evidently, then, to suppose that a probability assignment at a single trial is also an assertion about a frequency in many trials in the sense of the Bernoulli and de Moivre-Laplace limit theorems, is in general unjustified unless (1) the successive trials form an exchangeable sequence, and (2) the correlation of different trials is strictly zero.
However, there are other kinds of connections between probability and frequency; and maximum-entropy distributions have an exact and close relation to frequencies after all, as we shall see presently.


\subsection{Relation to Bayes' Theorem}
To prepare us to deal with some objections to the maximum-entropy solution \eqref{B13} we turn back to the basic product and sum rules of probability theory \eqref{A8}, \eqref{A9} derived by \cite{cox}{Cox} from requirements of consistency.
Just as any argument of deductive logic can be resolved ultimately into many syllogisms, so any calculation of inductive logic (i.e., probability theory) is reducible to many applications of these rules.

We stress that these rules make no reference to frequencies; or to any random experiment.
The numbers $p(A\given B)$ are simply a convenient numerical scale on which to represent degrees of plausibility.
As noted at the beginning of this work, it is the problem of determining initial numerical values by logical analysis of the prior information in more general cases than solved by Bernoulli and Laplace, that underlies our study.

Furthermore, in neither the statement nor the derivation of these rules is there any reference to the notion of a sample space.
In a formally qualitative sense, therefore, they may be applied to any propositions $A,B, C,\ldots$ with unambiguous meanings.
Their complete qualitative correspondence with ordinary common sense was demonstrated in exhaustive detail by Polya (\cite{polya}{1954}).

But in quantitative applications we find at once that merely defining two propositions, $A, B$ is not sufficient to determine any numerical value for $p(a\given B)$.
This numerical value depends not only on $A, B$, but also on which alternative propositions $A', A''$, etc. are to be considered if $A$ should be false; and the problem is mathematically indeterminate until those alternatives are fully specified.
In other words, we must define our ``sample space'' or ``hypothesis space'' before we have any mathematically well-posed problem.

In statistical applications (parameter estimation, hypothesis testing), the most important constructive rule is just the statement that the product rule is consistent; i.e., $p(AB\given C)$ is symmetric in $A$ and $B$, so $p(A \given BC)\cdot p(B \given C) = p(B\given AC)\cdot p(A\given C)$.
If $p(B\given C) \neq 0$, we thus obtain
\begin{equation}
	\label{B24}
	p(A \given BC) = p(A\given C) \frac{p(B\given AC)}{p(B\given C)}
\end{equation}
in which we may call $C$ the prior information, $B$ the conditioning information.
In typical applications, $C$ represents the general background knowledge or assumptions used to formulate the problem, $B$ is the new data of some experiment, and $A$ is some hypothesis being tested.
For example, in the Millikan oil-drop experiment, we might take $A$ as the hypothesis: ``the electronic charge lies in the interval $4.802 < \mathrm{e} < 4.803$,'' while $C$ represents the general assumed known laws of electrostatics and viscous hydrodynamics and the results of previous measurements, while $B$ stands for the new data being used to find a revised ``best'' value of $\mathrm{e}$.
Equation \eqref{B24} then shows how the prior probability $p(A\given C)$ is changed to the posterior probability $p(A\given BC)$ as a result of acquiring the new information $B$.

In this kind of application, $p(B\given AC)$ is a ``direct'' or ``sampling'' probability, since we reason in the direction of the causal influence, from an assumed cause $A$ to a presumed observable result $B$: and $p(A\given BC)$ is an ``inverse'' probability, in which we reason from an observed result $B$ to an assumed cause $A$.
On comparing with \eqref{A7} we see that \eqref{B24} is a more general form of Laplace's rule, in which we need not have an exhaustive set of possible causes.
Therefore, since \eqref{A7} is always called ``Bayes' theorem,'' we may as well apply the same name to \eqref{B24}.

At the risk--or rather the certainty--of belaboring it, we stress again that we are concerned here with inductive reasoning of any kind, not necessarily related to random experiments or any repetitive process.
On the other hand, nothing prevents us from applying the theory to a repetitive situation (i.e., $n$ tosses of a die); and propositions about frequencies $g_i$ are then just as legitimate pieces of data or objects of inquiry as any other prepositions.
Various kinds of connection between probability and frequency then appear, as mathematical consequences of \eqref{A8}, \eqref{A9}.
We have just seen one of them.
But now, could we have solved the Brandeis dice problem by applying Bayes' theorem instead of maximum entropy?
If so, how do the results compare? Friedman and Shimony (\cite{friedman}{1971}) (hereafter denoted \cite{friedman}{FS}) claimed to exhibit an inconsistency in the Principle of Maximum Entropy (hereafter denoted PME) by an argument which introduced a proposition $d_\epsilon$, so ill-defined that they tried to use it as
(1) a constraint in PME,
(2) a conditioning statement in Bayes’ theorem; and
(3) an hypothesis whose posterior probability is calculated.
Therefore, let us note the following.

If a statement $d$ referring to a probability distribution in space $S$ is testable (for example, if it specifies a mean value $\mean{f}$ for some function $f(i)$ defined on $S$), then it can be used as a constraint in PME; but it cannot be used as a conditioning statement in Bayes' theorem because it is not a statement about any event in $S$ or any other space.
Conversely, a statement $D$ about an event in the space $S^n$ (for example, an observed frequency) can be used as a conditioning statement in applying Bayes' theorem, whereupon it yields a posterior distribution on $S^n$ which may be contracted to a marginal distribution on $S$; but $D$ cannot be used as a constraint in applying PME in space $S$, because it is not a statement about any event in S, or about any probability distribution over $S$; i.e., it is not testable information in $S$.

At this point, informed students of statistical mechanics will be astonished at the suggestion that there is any inconsistency between application of PME in space $S$ and of Bayes’ theorem in $S^n$, since the former yields a canonical distribution, while the latter is just the Darwin-Fowler method, originally introduced as a rigorous way of justifying the canonical distribution!
The mathematical fact shown by this well-known calculation (Schrödinger, \cite{schrodinger}{1948}) is that, whether we use maximum entropy in space $S$ with a constraint fixing an average $\mean{f}$ over a \emph{probability distribution}, or apply Bayes' theorem in $S^n$ with a conditioning statement fixing a numerically equal average $f$ over \emph{sample values}, we obtain for large $n$ identical distributions in the space $S$.
The result generalizes at once to the case of several simultaneous mean-value constraints.

This not only illustrates--contrary to the claims of \cite{friedman}{FS}--the consistency of PME with the other principles of probability theory, but it shows what a powerful tool PME is; i.e., how much simpler and more convenient mathematically it is to use PME in statistical calculations if the distribution on $S$ is what we are seeking.
PME leads us directly to the same final result, without any need to go into a higher space $S^n$ and carry out passage to the limit $n\to\infty$ by saddle-point integration.

Of course, it is as true in probability theory as in carpentry that introduction of more powerful tools brings with it the obligation to exercise a higher level of understanding and judgment in using them.
If you give a carpenter a fancy new power tool, he \emph{may} use it to turn out more precise work in greater quantity; or he may just cut off his thumb with it.
It depends on the carpenter.

The \cite{friedman}{FS} article led to considerably more discussion (see the references collected with the \cite{friedman}{FS} one) in which severed thumbs proliferated like hydras; but the level of confusion about the points already noted is such that it would be futile to attempt any analysis of the \cite{friedman}{FS} arguments.

\cite{friedman}{FS} suggest that a possible way of resolving all this is ``to deny that the probability of $d_\epsilon$ can be well-defined.
Of course it cannot be; however, to understand the situation we need no ``deep and systematic analysis of the concept of reasonable degree of belief.''
We need only raise our standards of exposition to the same level that is required in any other application of probability theory; i.e., we must define our propositions and sample spaces with enough precision to make a determinate mathematical problem.

There is a more serious difficulty in trying to reply to these criticisms.
If \cite{friedman}{FS} dislike the maximum-entropy solution \eqref{B13} to this problem strongly enough to write three articles attacking it, then it would seem to follow that they prefer a different solution.
But what different solution?
One cannot form any clear idea of what is really troubling them, because in all these publications \cite{friedman}{FS} give no hint as to how, in their view, a more acceptable solution ought to differ from \eqref{B13}.


\subsection{The Rowlinson Criticism}
In sharp contrast to the \cite{friedman}{FS} criticisms is that of J. S. Rowlinson (\cite{rowlinson}{1970}), who considers the same dice problem but does offer an alternative solution.
For this reason, it is easy to give a precise quantitative reply to his criticism.

He starts with the all too familiar line: ``Most scientists would say that the probability of an event is (or represents) the frequency with which it occurs in a given situation.'' Likewise, a critic of Columbus could have written (\emph{after} he had returned from his first voyage): ``Most geographers would say that the earth is flat.''

Clarification of the centuries-old confusion about probability and frequency will not be achieved by taking votes; much less by quoting the philosophical writings of Leslie Ellis (\cite{ellis}{1842}).
Rather, we must examine the mathematical facts concerning the rules of probability theory and the different sample spaces in which probabilities and frequencies are defined.
We have seen, in the discussion following (B14) above, that anyone who glibly supposes that a probability in one space can be equated to a frequency in another, is assuming something which is not only not generally deducible from the principles of probability theory; it may stand in conflict, with those principles.

There is no stranger experience than seeing printed criticisms which accuse one of saying the exact opposite of what he \emph{has} said, explicitly and repeatedly.
Thus my bewilderment at Rowlinson's statement that I reject ``the methods used by Gibbs to establish the rules of statistical mechanics.''
I believe I can lay some claim to being the foremost current advocate and defender of Gibbs’ methods!
Anyone who takes the trouble to read Gibbs will see that, far from rejecting Gibbs' methods, I have adopted them enthusiastically and (thanks to the deeper understanding from Shannon) extended their range of application.

One of the major unsolved riddles of probability theory is: how to explain to another person \emph{exactly what is the problem being solved}?
It is well established that merely stating this in words does not suffice; repeatedly, starting with Laplace, writers have given the correct solution to a problem, only to have it attacked on the grounds that it is not the solution to some entirely different problem.
This is at least the tenth time it has happened to me.
As I tried to stress, the maximum entropy solution \eqref{B13} describes the state of knowledge in which we are given the enumeration of the six possibilities, the mean value $\mean{i} = 4.5$, and nothing else.
But Rowlinson proceeds to introduce models with an urn containing seven white and three black balls (or a population of urns with varying contents) from which one makes various numbers of random draws with replacement.
One expects that different problems will have different solutions.

In Rowlinson's Urn model, we perform Bernoulli trials five times, with constant probability of success $p=0.7$.
Then the numbers $s$ of successes is in $0\leq s\leq 5$, and the expected number is $\mean{s} = 5\times 0.7 = 3.5$.
Setting $ \mean{i} \equiv s+1 $, we have $1\leq i \leq 6$, $\mean{i} = 4.5$, the conditions stated in my dice problem.
Thus he offers as a counter-proposal the binomial distribution
\begin{equation}
	\label{B25}
	p'_i = \binom{5}{i-1} p^{i-1} (1-p)^{6-i}, \qquad 1\leq i\leq 6
\end{equation}
These numbers are
\begin{equation}
	\label{B26}
	\begin{split}
		\{p_1\ldots p_6\}
		&= \{0.00243,\, 0.02835,\, 0.1323,\\
		&\hphantom{= \{\ } 0.3087,\, 0.36015,\, 0.16807\}
	\end{split}
\end{equation}
and they yield an entropy $S'=1.413615$, $0.2$ unit lower than that of \eqref{B13}.
This lower entropy indicates that the urn model puts further constraints on the solution beyond that used in \eqref{B13}.
We see that these consist in the extreme values ($i=1,6$) receiving less probability than before (only one of $2^5 = 32$ possible outcomes can lead to $i = 1$, while ten of them yield $i = 3$, etc.).

Now if we \emph{knew} that the experiment consisted of drawing five times from an urn with just the composition specified by Rowlinson, the result \eqref{B25} would indeed be the correct solution.
But by what right does one \emph{assume} this elaborate model structure when it is not given in the statement of the problem?
One could, with equal right, assume any one of a hundred other specific models, leading to a hundred other counter-proposals.
But it is just the point of the maximum-entropy principle that it achieves ``objectivity'' of our inferences, in the sense that we base our predictions only on the information that we do, in fact, have; and carefully \emph{avoid} introducing any such gratuitous assumptions not warranted by our data.
Any such assumption is far more likely to impose false constraints than to happen, by luck, onto an unknown correct one (which would be like guessing the combination to a safe).

At this point, Rowlinson says, ``Those who favour the automatic use of the principle of maximum entropy would observe that the entropy of [our Eq.~\eqref{B25}], $1.4136$, is smaller than that of [\ref{B13}], and so say that in proposing [\ref{B25}] as a solution, `information' has been assumed for which there is no justification.''
We do indeed say this, although Rowlinson simply rejects it out of hand without giving a reason.
So to sustain our claim, let us calculate explicitly just how much Rowlinson's solution assumes without justification.

To clarify what is meant by ``assuming information,'' suppose that an economist, Mr. A, is trying to forecast future price trends for some commodity.
The condition of next week's market cannot be known with certainty, because it depends on intentions to buy or sell hidden in the minds of many different individuals.
Evidently, a rational method of forecasting must somehow take account of all these unknown possibilities.
Suppose that Mr. A's data are found to be equally compatible with $100$ different possibilities.
If he arbitrarily picked out $10$ of these which happened to suit his fancy, and based his forecast only on them, ignoring the other $90$, we should certainly consider that Mr. A was guilty of an egregious case of assuming information without justification.
Our present problem is similar in concept, but quite different in numerical values.

We have stressed that, fundamentally, the maximum-entropy solution \eqref{B13} describes only a state of knowledge about a single trial, and is not an assertion about frequencies.
But Rowlinson, as noted, also rejects this distinction and wants to judge the issue on the grounds of frequencies.
Very well; let us now bring out the frequency connection that a maximum-entropy distribution does, after all, have (and which, incidentally, as pointed out in my Brandeis lectures, from which Rowlinson got this dice problem).

In $N$ tosses, a set of observed frequencies $\{g_i\}= \{N_i/N\}$ (called $g$ to avoid collision with previous notation) can be realized in
\begin{equation}
	\label{B27}
	W = \frac{N!}{(N_{g_1})! (N_{g_2})! \cdots (N_{g_6})!}
\end{equation}
different ways. As we noted from Boltzmann's work, Eq.~\eqref{A14}, the Stirling approximation to the factorials yields an asymptotic formula
\begin{equation}
	\label{B28}
	\log W \sim NS
\end{equation}
where
\begin{equation}
	\label{B29}
	S = -\sum_{i=1}^{6} g_i \log g_i
\end{equation}
is the entropy of the observed frequency distribution.
Given two different sets of frequencies $\{g_i\}$ and $\{g'_i\}$, the ratio: (number of ways $g_i$ can be realized)/(number of ways $g'_i$ can be realized) is given by an asymptotic formula
\begin{equation}
	\label{B30}
	\frac{W	}{W'} \sim A \exp[N(S-S')] \left(1 + \frac{B}{N} + \mathcal{O}\left(N^{-2}\right)\right)
\end{equation}
where
\begin{equation}
	\label{B31}
	A \equiv \prod_{i} (g'_i/g_i)^{1/2}
\end{equation}
\begin{equation}
	\label{B32}
	B \equiv \frac{1}{12}\sum_{i} \left(\frac{1}{g'_i} - \frac{1}{g_i}\right)
\end{equation}
are independent of $N$, and represent corrections from the higher terms in the Stirling approximation.
We write them down only to allay any doubts about the accuracy of the numbers to follow.
In all cases considered here it is easily seen that they have no effect on our conclusions, and only the exponential factor matters.

Rowlinson mentions an experiment involving $20\,000$ throws of a die, to which we shall return later; but in ¢he present comparison this leads to numbers beyond human comprehension.
To keep the results more modest, let us assume only $N = 1000$ throws.
If we take $\{g_i\}$ as the maximum-entropy distribution \eqref{B13} and $\{g'_i\}$ as Rowlinson's solution \eqref{B26}, we find $A = 0.2$, $B = 50$, $S-S' = 0.200$; and thus, with $N = 1000$,
\begin{equation}
	\refstepcounter{equation}
	\label{B34}
	\frac{W	}{W'} = 1.52 \times 10^{86}
\end{equation}
Both distributions agree with the datum $\mean{i} = 4.5$; but for every way in which Rowlinson's distribution can be realized, there are over $10^{86}$ ways in which the maximum entropy distribution can be realized (the age of the universe is less than $10^{18}$ seconds).
It appears that information was indeed ``assumed for which there is no justification.''

This example should help to give us a proper respect for just what we are accomplishing when we maximize entropy.
It shows the magnitude of the indiscretion we commit if we accept a distribution whose entropy is $0.2$ unit less than the maximum value compatible with our data.
In this example, to accept any distribution whose entropy is as much as $0.005$ below the maximum value, would be to ignore over $99$ percent of all possible ways in which the average $\mean{i} = 4.5$ could be realized.
For reasons unexplained, Rowlinson seizes upon the particular value $p_1 = 0.05435$ from the maximum-entropy solution \eqref{B13}, and asks: ``But what basis is there for trusting in this last number?'' but fails to ask the same question about his own very different result $p'_1 = 0.00243$.
Since it is so seldom that one is able to give a quantitative reply to a rhetorical question, we should not pass up this opportunity.


\subsubsection{Answer to the Rhetorical Question}
Let us, as before, count up the number of possibilities compatible with the given data.
In the original problem we were to find $\{p_1\ldots p_n\}$ so as to maximize $H=-\sum p_i \log p_i$ subject to the constraints $\sum p_i = 1$, $\mean{i} = \sum ip_i$, a specified numerical value.
If now we impose the additional constraint that $p_1$, is specified, we can define conditional probabilities
\begin{equation}
	\label{B35}
	p'_i = \frac{p_i}{1 - p_i}, i = 2, 3, \ldots, n
\end{equation}
with entropy
\begin{equation}
	\label{B36}
	H' = - \sum_{i=2}^{n} p'_i \log p'_i
\end{equation}
These quantities are related by Shannon's basic functional equation
\begin{equation}
	\label{B37}
	H(p_1\ldots p_n) = H(p_1, 1-p_1) + (1-p_1)H' (p'_2\ldots p'_n)
\end{equation}
and so, maximizing $H$ with $p_1$ held fixed is equivalent to maximizing $H'$.
We have the reduced maximum entropy problem: maximize $H'$ subject to
\begin{equation}
	\label{B38}
	\sum_{i=2}^{n} p'_i = 1
\end{equation}
\begin{equation}
	\label{B39}
	\mean{i}' = \sum_{i=2}^{n} i p'_i = 1 + \frac{\mean{i} - 1}{1 - p_1}
\end{equation}
The solution proceeds as before, but now the maximum attainable entropy is a function $H_{\mathrm{max}} = S(p_1,\mean{i})$ of the specified value of $p_1$, as well as $\mean{i}$.
The maximum of $S(p_1, 4.5)$ is of course the previous value \eqref{B14} of $1.61358$, attained at the maximum-entropy value $p_1 = 0.05435$.
Evaluating this also for Rowlinson's $p'_1$, I find $S(p'_1, 4.5) = 1.55716$, lower by $0.05642$ units.
By \eqref{B30} this means that, in $1000$ tosses, for every way in which Rowlinson's value could be realized, \emph{regardless of all other frequencies except for the constraint} $\mean{i} = 4.5$, there are over $10^{24}$ ways in which the maximum-entropy frequency could be realized.

We may give a more detailed answer: expanding $S(p_1, 4.5)$ about its peak, we find that as we depart from $0.05435$, the number of ways in which the frequency $g_1$, could be realized drops off like
\begin{equation}
	\label{B40}
	\exp[-14\,000 (g_1 - 0.05435)^2]
\end{equation}
and so, for example, for $99$\% of all possible ways in which the average $\mean{i} = 4.5$ can be realized, $g_1$ lies in the interval $(0.05435 +\pm 0.0153)$.

This would seem to be an adequate answer to the question, ``But what basis is there for trusting in this number?''
I stress that the numerical results just given are \emph{theorems}, involving only a straightforward counting of the possibilities allowed by the given data.
Therefore they stand independently of anybody's personal opinions about either dice or probability theory.

However, it is necessary that we understand very clearly the meaning of these frequency connections.
They concern only the number of \emph{possible} ways in which certain frequencies $\{g_i\}$ could be realized, compatible with our constraints.
They do not assert that the maximum-entropy frequencies will be observed in a real experiment; indeed, neither the Principle of Maximum Entropy nor any other principle of probability theory can predict with certainty what will happen in a real experiment.
The correct statement is rather: the frequency distribution $\{g_i\}$ with maximum entropy calculated from certain constraints is overwhelmingly the most likely one to be observed in a real experiment, provided that the physical constraints operative in the experiment are the same as those assumed in the calculation.

In our mathematical formalism, a ``constraint'' is some piece of information that leads us to modify a probability distribution; in the case of a mean value constraint, by inserting an exponential factor $\exp[-\lambda f(x)]$ with an adjustable Lagrange multiplier $\lambda$.
It is perhaps not yet clear just what we mean by ``constraints'' in a physical experiment.
Of course, by these we do not mean the gross constraining linkages by levers, cables, and gears of a mechanics textbook, but something more subtle.
In our applications, a ``physical constraint'' is any physical influence that exerts a systematic tendency--however slight-~on the outcome of an experiment.
We give some specific examples of physical constraints in die tossing below.

From the above numbers we can understand the success of the work of J.~C.~Keck and R.~D.~Levine reported here.
I am sure that their results must seem like pure magic to those who have not understood the maximum-entropy formalism.
To find a distribution of populations over $20$ molecular energy levels might seem to require $19$ independent pieces of data.
But if one knows, from approximate rate coefficients or from past experience, which constraints exist (in practice, even if only the one or two most important ones are taken into account), one can make quite confident predictions of distributions over many levels simply by maximizing the entropy.

In fact, most frequency distributions produced in real experiments are maximum-entropy distributions, simply because these can be realized in so many more ways than can any other.
As $N\to\infty$, the combinatorial factors become so sharply peaked at the maximum entropy point that to produce any appreciably different distribution would require very effective physical constraints.
Any statistically significant departure from a maximum-entropy prediction then constitutes strong--and if it persists, conclusive--evidence of the existence of new constraints that were not taken into account in the calculation.
Thus the maximum-entropy formalism has the further ``magical'' property that it provides the most efficient procedure by which, if unknown constraints exist, they can be discovered. But this is only an updated version of the process noted in Section~\ref{sec:A} by which Laplace discovered new systematic effects.

It is, perhaps, sufficiently clear from this how much a Physical Chemist has to gain by understanding, rather than attacking, maximum entropy methods.

But we still have not dealt with the most fundamental misunderstandings in the Rowlinson article.
He turns next to the shape of the maximum-entropy distribution \eqref{B13}, with another rhetorical question: ``--- is there anything in the mechanics of throwing dice which suggests that if a die is not true the probabilities of scores $1,2,\ldots 6$, should form the geometrical progression [our Eq.~\eqref{B13}]?''
He then cites some data of Wolf on $20\,000$ throws of a die which gave an average $\mean{i}= 3.5983$, plots the observed frequencies against the maximum-entropy distribution based on that constraint, and concludes that ``departures from the random value of $1/6$ bear no resemblance to those calculated from the rule of maximum entropy.
What is clearly wrong with the indiscriminate use of this rule, and of the older rules from which it stems, is that they ignore the physics of the problem.''

We have here a total, absolute misconception about every point I have been trying to explain above.
If Wolf's data depart significantly from the maximum-entropy distribution based only on the constraint $\mean{i} = 3.5983$, then the proper conclusion is not that maximum entropy methods ``ignore the physics'' but rather that the maximum entropy method brings out the physics by showing us that another physical constraint exists beyond that used in the calculation.
Unable to see the new physical information here revealed, he lashes out blindly against the principle that has revealed it.

Therefore, let us now give an analysis of Wolf's dice data showing just what things maximum entropy can give us here, if we only open our eyes to them.


\subsubsection{Wolf's Dice Data}
In the period roughly 1850--1890, the Zurich astronomer R. Wolf conducted and reported a mass of ``random experiments.''
An account is given by Czuber (\cite{czuber}{1908}).
Our present concern is with a particular die (identified as ``Weiszer Würfei'' in Czuber's two-way table, \emph{loc. cit} p. 149) that was tossed $20\,000$ times and yielded the aforementioned mean value $\mean{i} = 3.5983$.
We shall look at all details of the data presently, but first let us note a few elementary things about that ``ignored'' physics.

We all feel intuitively that a perfectly symmetrical die, fairly tossed, ought to show all faces equally often (but that statement is really circular, since there is no other way to define a ``fair'' method of tossing; so, suppose that by experimenting on a die known to be true, we have found such a fair method, and we continue to use it).
The uniform frequency distribution $\{g_i = 1/6, 1\leq i\leq 6\}$ then represents the nominal ``unconstrained'' situation of maximum possible entropy $S = \log 6$.
Any imperfection in the die may then give rise to a ``physical constraint'' as we have defined that term.
A little physical common sense can anticipate what these imperfections are likely to be.

The most obvious imperfection is that different faces have different numbers of spots.
This affects the center of gravity, because the weight of ivory removed from a spot is obviously not (in any die I have seen) compensated by the paint then applied.
Now the numbers of spots on opposite faces add up to seven.
Thus the center of gravity is moved toward the ``$3$'' face, away from ``$4$'', by a small distance $\epsilon$ corresponding to the one spot discrepancy.
The effect of this must be a slight frequency difference which is surely, for very small $\epsilon$, proportional to $\epsilon$:
\begin{equation}
	\label{B41}
	g_4 - g_3 = \alpha \epsilon
\end{equation}
where the coefficient $\alpha$ would be very difficult to calculate, but could be measured by experiments on dies with known $\epsilon$.
But the $(2{-}5)$ face direction has a discrepancy of three spots, and $(1{-}6)$ of five.
Therefore, we anticipate the ratios:
\begin{equation}
	\label{B42}
	(g_4 - g_3) : (g_5 - g_2) : (g_6 - g_1) = 1 : 3 : 5
\end{equation}
But this says only that the spot frequencies vary linearly with $i$:
\begin{equation}
	\label{B43}
	g_i =  \frac{1}{6} + \alpha \epsilon f_1(i), \qquad 1 \leq i \leq 6
\end{equation}
where
\begin{equation}
	\label{B44}
	f_1(i) \equiv (i - 3.5)
\end{equation}
The spot imperfections should then lead to a small linear skewing favoring the ``$6$.''
This is the most obvious ``physical constraint,'' and it changes the expected number of spots to
\begin{equation}
	\label{B45}
	\mean{i} = \sum i g_i = 3.5 + 17.5\alpha\epsilon
\end{equation}
or, to state it more suggestively, the function $f_1(i)$ acquires a non-zero expectation
\begin{equation}
	\label{B46}
	\mean{f_1} = 17.5 \alpha\epsilon
\end{equation}
Now, what is the next most obvious imperfection to be expected?
Evidently, it will involve departure from a perfect cube, the specific kind depending on the manufacturing methods; but let us consider only the highest quality die that a factory would be likely to make.
If you were assigned the job of making a perfect cube of ivory, how would you do it with equipment Likely to be available in a Physics Department shop or a small factory?

I think you would head for the milling machine, and mount your lump of ivory on a dividing head clamped to the work table with axis vertical.
The first cut would be with an end mill, making the ``top'' face of the die.
The construction of the machine guarantees that this will be accurately plane.
Then you use side cutters to make the four side faces.
For the finish cuts you will move the work table only in the direction of the cut, rotating the dividing head $90$° from one face to the next.
The accuracy of the equipment guarantees that you now have five of the faces of your cube, all very accurately plane and all angles accurately $90$°, the top face accurately square.

But now the trouble begins; to make the final ``bottom'' face you have to remove the work from its mount, place it upside down on the table, and go over it with the end mill.
Again, the construction of the machine guarantees that this final face will be accurately plane and parallel to the ``top;'' \emph{but it will be practically impossible to adjust the work table height so accurately that the final dimension is exactly equal to the other two}.
Of course, a skilled artisan with a great deal more time and equipment could do better; but this would run up the cost of manufacture for something that would never be detected in use.
For factory production, there would be no motivation to do better than we have described.

Thus, the most likely geometrical imperfection in a high quality die is not lack of parallelism or of true $90$° angles, put rather that one dimension will be slightly different from the other two.

Again, it is clear what kind of effect this will have on frequencies.
Suppose the die comes out slightly ``oblate,'' the $(1{-}6)$ dimension being shorter than the $(2{-}5)$ and $(3{-}4)$ by some small amount $\delta$.
If the die were otherwise perfect, this would evidently increase the frequencies $g_1$, $g_6$ by some small amount $\beta\delta$, and decrease the other four to keep the sum equal to unity, where $\beta$ is another coefficient hard to calculate but measurable. The result can be stated thus: the function
\begin{equation}
	\label{B47}
	f_3(i) =
	\begin{cases}
		+2, \quad i = 1, 6\\
		-1, \quad i = 2, 3, 4, 5
	\end{cases}
\end{equation}
defined on the sample space, acquires a non-zero expectation
\begin{equation}
	\label{B48}
	\mean{f_3} = 6\beta\delta
\end{equation}
and the frequencies are
\begin{equation}
	\label{B49}
	\begin{split}
		g_i
			&= \frac{1}{6} + \frac{1}{2} \beta\delta f_3(i)\\
			&= \frac{1}{6}\left(1 + 3\beta\delta f_3(i)\right)
	\end{split}
\end{equation}
If now both imperfections are present, since the perturbations are so small we can in first approximation just superpose their effects:
\begin{equation}
	\label{B50}
	g_i \approx \frac{1}{6} [1 + 6\alpha\epsilon f_1(i)][1 + 3\alpha\epsilon f_3(i)]
\end{equation}
But this is hardly different from
\begin{equation}
	\label{B51}
	g_i = \frac{1}{6}[6\alpha\epsilon f_1(i) + 3\beta\delta f_3(i)]
\end{equation}
and so a few elementary physical common-sense arguments have led us to something which begins to look familiar.

If we had done maximum entropy using the constraints \eqref{B46}, \eqref{B48}, we would find a distribution proportional to $\exp[-\lambda_1 f_1(i) - \lambda_3 f_3(i)]$, so that \eqref{B51} is a maximum-entropy distribution based on those constraints.
We see that the Lagrange multiplier by which any information constraint is coupled into our probability distribution, is just a measure of the strength of the physical constraint required to realize a numerically equal frequency distribution:
\begin{align}
	&\lambda_1 = -6\alpha\epsilon\label{B52}\\
	&\lambda_3 = -3\beta\delta\label{B53}
\end{align}
and if our die has no other imperfections beyond the two noted, then it is overwhelmingly more likely to produce the distribution \eqref{B51} than any other.

If the observed frequencies show any statistically significant departure from \eqref{B51}, then we have extracted from the data evidence of a third imperfection, which probably would have been totally invisible in the raw data; i.e., only when we have used the maximum entropy principle to ``subtract off'' the effect of the stronger influences, can we hope to detect a weaker one.

Our program for the maximum-entropy analysis of the die--or any other random experiment--is now defined except for the final step; how we decide whether a discrepancy is ``statistically significant?''

The reader is cautioned that in all this discussion relating to Rowlinson we are being careless about distinctions between probability and frequency, because Rowlinson himself makes no distinction between them, and trying to correct this at every point quickly became tedious.
The following analysis should be restated much more carefully to bring out the fact that it is only a very special case, although to the ``frequentist'' it appears to be the general case.

We have some ``null hypothesis'' $H_0$, about our die, that leads us to assign the probabilities $\{p_1\ldots p_6\}$.
We obtain data from $N$ tosses, in which the observed frequencies are $\{g_1 = N_i/N, 1\leq i \leq6\}$.
If the numbers $\{g_1\ldots g_6\}$ are sufficiently close to $\{p_1\ldots p_6\}$ we shall say the fit is satisfactory; the null hypothesis is consistent with our data, and so there is no need, as far as this experiment indicates, to seek a better hypothesis.
But how close is "close?" How do we measure the ``distance'' between the two distributions; and how large may that distance be before we begin to doubt the null hypothesis?

Early in this Century, Karl Pearson invented an intuitive, \emph{ad hoc} procedure, called the Chi-squared test, to deal with this problem, which has been since widely adopted.
Here we calculate the quantity
\begin{equation}
	\label{B54}
	\chi^2 = N\sum_{i=1}^{6} \frac{(g_i - p_i)^2}{p_i}
\end{equation}
and if it is greater than a certain ``critical value'' given in Tables, we reject the null hypothesis.
In the present case (six categories, five ``degrees of freedom'' after normalization), the critical value at the conventional $5$\% significance level is
\begin{equation}
	\label{B55}
	\chi^2_c = 11.07
\end{equation}
which means that, if the null hypothesis is true there is only a $5$\% chance of seeing a value greater than $\chi^2_c$.
The critical value is independent of $N$, because for a frequentist who believes that $p_i$ is an assertion of a limiting frequency in the sense of the de Moivre-Laplace limit theorem \eqref{A4}, if $H_0$ is true, then the deviations should fall off as $\abs{g_i - p_i} = O\left(N^{-1/2}\right)$.
A more careful approach shows that this holds only if our model is an exchangeable sequence with zero correlations; and even in this case the $\chi^2$ criterion of ``closeness'' has no theoretical justification (i.e., no uniqueness property) in the basic principles of probability theory.

In fact, for the case of independent exchangeable trials, there is a criterion with a direct information-theory justification (Kullback, \cite{kullback}{1959}) in the ``minimum discrimination information
statistic''
\begin{equation}
	\label{B56}
	\psi = N \sum_{i=1}^{6} g_i \log (g_i/p_i)
\end{equation}
and the numerical value of $\psi$, rather than $\chi^2$, will lead us to inferences directly justifiable by Bayes’ theorem.
If the deviations $(g_i-p_i)$ are large, these criteria can be very different.


However, by a lucky mathematical accident, if the deviations are small (as we already know them to be for our dice problem) an expansion in powers of $(g_i- p_i)$ {in the logarithm, write $g/p = 1+ (g-p)/g + (g-p)^2/gp]$ yields
\begin{equation}
	\label{B57}
	\psi = \frac{1}{2} \chi^2 + O\left(N^{-1/2}\right)
\end{equation}
the neglected terms falling off as indicated, \emph{provided} that $\abs{g_i - p_i} = O(N^{-{1}/{2}})$.
The result is that in our problem, from a pragmatic standpoint it doesn't matter whether we use $\chi^2$ or $\psi$.
So I shall apply the $\chi^2$ test to Wolf's data, because it is so much more familiar to most people.

Wolf's empirical frequencies $\{g_i\}$ are given in the second column of Table~\ref{tab:1}.
As a first orientation, let us test then against the null hypothesis $\{H_0 : p_i = 1/6, 1\leq i\leq 6\}$ of a uniform die.
We find the result
\begin{equation}
	\label{B58}
	\chi_0^2 = 271
\end{equation}
over twenty times the critical value \eqref{B55}.
The hypothesis $H_0$ is decisively rejected.

Next, let us follow Rowlinson by considering a new hypothesis $H_1$ which prescribes the maximum-entropy solution based on Wolf's average $\mean{i} = 3.5983$, or,
\begin{equation}
	\label{B59}
	\mean{f_1(i)} = 0.0983
\end{equation}
This will give us a distribution $p_i \sim \exp[-\lambda f_1(i)]$.

From the partition function \eqref{B11} with this new datum we find $A = 0.03373$ and the probabilities given in the third column of Table~\ref{tab:1}.
The fourth column gives the differences $\Delta_i = g_i - p_i$, while in the fifth we list the partial contributions to Chi-squared:
\begin{equation*}
	c_i = 20\,000 \frac{(g_i - p_i)^2}{p_i}
\end{equation*}
which adds up to the value
\begin{equation}
	\label{B60}
	\chi^2_1 = 199.4
\end{equation}
The fit is improved only slightly; and $H_1$ is also decisively rejected.

\begin{table}[htbp]
	\centering
	{
	\lfstyle
	\begin{tabular}{c c c c c}
		\toprule
		$i$ & $g_i$ & $p_i$ & $\Delta_i$ & $c_i$\\
		\midrule
		1 & 0.16230 & 0.15294 & $+$\,0.0094 & 11.46\\
		2 & 0.17245 & 0.15818 & $+$\,0.0143 & 25.75\\
		3 & 0.14485 & 0.16361 & $-$\,0.0188 & 43.02\\
		4 & 0.14205 & 0.16922 & $-$\,0.0272 & 87.25\\
		5 & 0.18175 & 0.17502 & $+$\,0.0067 & 5.18\\
		6 & 0.19660 & 0.18103 & $+$\,0.0156 & 26.78\\
		\cmidrule{5-5}
		&         &         &         & 199.43\\
		\bottomrule
	\end{tabular}
}
\caption{One Constraint}
\label{tab:1}
\end{table}

At this point, Rowlinson wants to reject not only $H_1$, but also the whole principle of maximum entropy.
But now I stress still another time what the principle is really telling us: \emph{a statistically significant deviation is evidence of a new physical constraint; and the nature of the deviation gives us a clue as to what that constraint is}.
After subtracting off, by maximum entropy, the deviation attributable to the first constraint, the nature of the most important remaining one is revealed.
Indeed, from a glance at the deviations $\Delta_i = g_i - p_i$ the answer leaps out at us; Wolf's die was slightly ``prolate,'' the $(3{-}4)$ dimension being greater than the $(2{-}5)$ and $(1{-}6)$ ones.
So, instead of \eqref{B47}, the new constraint is
\begin{equation}
	\label{B61}
	f_2(i) =
	\begin{cases}
		+1, \quad i = 1, 6\\
		-2, \quad i = 2, 3, 4, 5
	\end{cases}
\end{equation}
and Wolf's data yield the result
\begin{equation}
	\label{B62}
	\mean{f_2} = 0.1393
\end{equation}
So now let us subtract off, by maximum entropy, the effect of both of these constraints; and thus discover whether Wolf's die had a third imperfection.

With the two constraints \eqref{B59}, \eqref{B62} we have two Lagrange multipliers and a partition function
\begin{equation}
	\label{B63}
	\begin{split}
		Z(\lambda_1, \lambda_2)
		&= \sum_{i=1}^{6} \exp[-\lambda_1f_1(i) - \lambda_2f_2(i)]\\
		&= x^{-5/2}  y(1 + x) (1 + x^4 + x^2y^{-3})
	\end{split}
\end{equation}
where $x \equiv \exp (-\lambda_1)$, $y \equiv \exp(-\lambda_2)$.
The maximum-entropy probabilities are then
\begin{equation}
	\label{B64}
	\{p_1\ldots p_6\} = Z^{-1} x^{-5/2}  y\big\{1,\, x,\, x^2 y^{-3},\, x^3y^{-3},\, x^4,\, x^5\big\}
\end{equation}
Writing out the constraint equations \eqref{B4} and eliminating $y$ from them, we find that $x$ is determined by
\begin{equation}
	\label{B65}
	\begin{split}
		(6F_1 - 4F_2 - 11)x^5 &+ (6F_1 - 4F_2 - 5)x^4 + (6F_1 + 4F_2 + 5)x \\
		&+ (6F_1 + 4F_2 + 11) = 0
	\end{split}
\end{equation}
or, with Wolf's numerical values \eqref{B59}, \eqref{B62},
\begin{equation}
	\label{B66}
	5.4837 x^5 + 2.4837 x^4 - 3.0735 x - 6.0735 = 0
\end{equation}
This has only one real root, at $x = 1.032233$, from which we have $\lambda_1 = -0.0317244$, $y = 1.074415$, $\lambda_2 = -0.0717764$.
The new maximum-entropy probabilities are given in Table~\ref{tab:2}, which contains the same information as Table~\ref{tab:1}, but for the new hypothesis $H_2$.

\begin{table}[htbp]
	\centering
	{
	\lfstyle
	\begin{tabular}{c c c c c}
		\toprule
		$i$ & $g_i$ & $p_i$ & $\Delta_i$ & $c_i$\\
		\midrule
		1 & 0.16230 & 0.16433 & -0.0002 & 0.502\\
		2 & 0.17245 & 0.16963 & +0.0028 & 0.938\\
		3 & 0.14485 & 0.14117 & +0,0037 & 1.919\\
		4 & 0.14205 & 0.14573 & -0.0037 & 1.859\\
		5 & 0.18175 & 0.18656 & -0.0048 & 2.480\\
		6 & 0.19660 & 0.19258 & +0.0040 & 1.678\\
		\cmidrule{5-5}
		  &         &         &         & 9.375\\
		\bottomrule
	\end{tabular}
}
	\caption{Two Constraints}
	\label{tab:2}
\end{table}
We see that the second constraint has greatly improved the fit.
Chi-squared has been reduced to
\begin{equation}
	\label{B67}
	\chi^2_2 = 9.375
\end{equation}
This is less than the critical value $11.07$, so there is now no statistically significant evidence for any further imperfections i.e., if the given $p_i$ were the ``exact'' values, it is reasonably likely that the distribution gy would deviate from $p_i$ by the observed amount, by chance alone.
Or, to put it in a way perhaps more appropriate to this problem, if the die were tossed another $20\,000$ times, we would not expect the frequencies $g_i$ to be repeated exactly; the new frequencies $g_i'$, might reasonably be expected to deviate from the first set $g_i$ by about as much as the distributions $g_i'$, $p_i$ differ.

That this is reasonable can be seen directly without calculating Chi-squared.
For if the result $i$ is obtained $n_i$ times in $N$ tosses, we might expect this to fluctuate in successive
repetitions of the whole experiment by about $\pm\sqrt{n_i}$.
Thus the observed frequencies $g_i = n_i/N$ should fluctuate by about $\Delta g_i = \pm \sqrt{n_i/N}$;
for $g_i = 1/6$, $N = 20\,000$, this gives $\Delta g_i = 0.0029$.
But this is just of the order of the observed deviations $\Delta_i$.
Therefore, it would be futile to search the $\{\Delta_i\}$ of Table~\ref{tab:2} for a third imperfection.
Not only does their distribution fail to suggest any simple hypothesis; if the die were tossed another $20\,000$ times, in all probability the new $\Delta_i'$ would be entirely different.
With our two-parameter hypothesis $H_2$ we are down ``in the noise'' of random variations, and any further systematic influences are too small to be seen unless we go up to a million tosses, by which time the die will be changed anyway by wear.

A technical point might be raised by Statisticians: ``You have estimated two parameters $\lambda_1, \lambda_2$ from the data; therefore you should use the test for three degrees of freedom rather than five.''
This reduction is appropriate if the parameters are chosen by the \emph{criterion} of minimizing $\chi^2$.
That is, if we choose them for the express purpose of making $\chi^2$ small and still fail to do so, it does not speak well for the hypothesis and a penalty is in order.
But our parameters were chosen by a criterion that took no note of $\chi^2$; and therefore the proper question is only: ``How well does the result fit the data?'' and not: ``How did you find the parameters?''
Had we chosen our parameters to minimize $\chi^2$, we, would have found a still lower value; but one that is not relevant to the point being made here, which is the performance of the \emph{maximum entropy} criterion, as advocated long before this die problem was thought of.

The maximum entropy method with two Lagrange multipliers thus successfully determines a distribution with five independent quantities.
The ``ensemble'' canonical with respect to the constraints $f_1(i)$, $f_2(i)$ describing the two imperfections that common sense leads us to expect in a die, agrees with Wolf's data about as well as can be hoped for in a statistical problem.

It was stressed above that in this theory the connections between probability and frequency are loosened and we noted, in the discussion following \eqref{B40}, that the connections remaining are now theorems rather than conjectures.
As we now see, they are not loosened enough to hamper us in dealing with real random experiments.
If we had been given only the two constraints \eqref{B59}, \eqref{B62} we could have reproduced, by maximum entropy, all of Wolf's frequency data.

This is an interesting caricature of the results of Keck and Levine, and shows again how much our critics would gain by understanding, rather than attacking, this principle.
Far from ``ignoring the physics,'' it leads us to concentrate our attention on the part of the physics that is \emph{relevant}.
Success in using it does not require that we take into account all dynamical details; it is enough if we can recognize, whether by common-sense analysis or by inspection of data, \emph{what are the systematic influences at work, that represent the ``physical constraints?''}
If by any means we can recognize these, maximum entropy then takes over and supplies the rest of the solution, which does not depend on dynamical details but only on counting the possibilities.

In effect, then, by subtracting off the systematic effects we reduce the problem to Bernoulli's ``equally possible'' cases; the deviations $\Delta_i$ from the canonical distribution that remain in Table~\ref{tab:2} are the same as the deviations from $p_i = 1/6$ that we would expect if the die had no imperfections.

Success of our predictions is not guaranteed in advance, as Rowlinson supposed it should be when he wanted to reject the entire principle at the stage of Table~\ref{tab:1}.
But this supposition merely reflects his rejection, at the very outset, of the distinction between probability and frequency that I keep stressing.
If one is not moved by theoretical arguments for that distinction, we now see a pragmatic reason for it.
The \emph{probabilities} $p_i$ in Table~\ref{tab:1} are an entirely correct description of our \emph{state of knowledge} about a single toss, when we know about only the constraint $f_1(i)$.
It is a theorem that they are also numerically equal to the \emph{frequencies} which could happen in the greatest number of ways if no other physical constraint existed.
But our probabilities will agree with measured frequencies only when we have recognized and put into our equations the constraints representing \emph{all} the systematic influences at work in the real experiment.

This, I submit, is exactly as it should be in a statistical theory; at no point are we ever justified in claiming that our predictions \emph{must} be right; only that, in order to make any better ones we should need more information than was given.
It is when a theory purports to do more than this (by failing to recognize the distinction between probability and frequency) that it may be charged with promising us something for nothing.

Since the fit is now satisfactory, the above values of $\lambda_1$, $\lambda_2$ give us the numerical values of the systematic influences in Wolf's experiment: from \eqref{B52}, \eqref{B53} we have
\begin{align}
	\alpha\epsilon = \frac{0.03172}{6} = 0.0053\label{B68}\\
	\beta\delta = \frac{0.07178}{3} = 0.024\label{B69}
\end{align}
So, if today some enterprising person at Monte Carlo or Las Vegas will undertake to measure for us the coefficients $\alpha$, $\beta$, then we can determine--100 years after the fact--just how far (in terms of its nominal dimensions) the center of gravity of Wolf's die was displaced (presumably by excavation of the spots), and how much longer it was in the $(3{-}4)$ direction than in the $(2{-}5)$ or $(1{-}6)$.
We can also certify that it had no other significant imperfections (at least, none that affected its frequencies).
Note, however, that $\alpha$, $\beta$ are not, strictly speaking, physical constants only of the die; a little further common-sense reasoning makes it clear that they must depend also on how the die was tossed;
for example, tossing it with a large angular momentum about a $(3{-}4)$ axis will decrease the effect of the $f_1(i)$ constraint, while if it spins about the $(1{-}6)$ axis the effect of $f_2(i)$ will be less; and with a (2{-}5) spin axis both constraints will be weakened.

Indeed, as soon as the die is unsymmetrical, all sorts of physical conditions that were irrelevant for a perfectly symmetrical one, become relevant.
The frequencies will surely depend not only on its center of gravity but also on all the second moments of its mass distribution, the sharpness of its edges, the smoothness, elasticity, and coefficient of friction of the table, etc.

However, we conjecture that $\alpha$, $\beta$ depend very little on these factors within the small range of conditions usually employed (i.e., small angular momentum in tossing, etc.); and suspect that in that range the coefficient $\alpha$ is already well known to those who deal with loaded dice.

I really must thank Rowlinson for giving us (albeit unintentionally) such a magnificent test case by which the nature and power of the Principle of Maximum Entropy can be demonstrated, in a context entirely removed from the conceptual problems of quantum theory.
And indeed, all the criticisms he made were richly deserved; for he was not, after all, criticizing the Principle of Maximum Entropy; only a gross misunderstanding of it.
Rowlinson's criticisms were, however, taken up and extended by Lindhard (\cite{lindhard}{1974}); in view of the long commentary above we may leave it as an exercise for the reader to deal with his arguments.


\subsection{The Constraint Rule}
There is a further point of logic about our use of maximum entropy that has troubled some who are able to see the distinction between probability and frequency.
In imposing the mean-value constraint \eqref{B1} we are simply appropriating a \emph{sample} average obtained from $N$ measurements that yielded $f_j$ on the $j$'th observation:
\begin{equation}
	\label{B70}
	F = \overline{f} = \frac{1}{N} \sum_{j=1}^{N} f_j
\end{equation}
and equating it to a \emph{probability} average
\begin{equation}
	\label{B71}
	\mean{f} = \sum_{i=1}{n} p_i f(x_i)
\end{equation}
Is there not an element of arbitrariness about this?
A cynic might say that after all these exhortations about the distinction between probability and frequency, we proceed to confuse them after all, by using the word ``average'' in two quite different senses.

Our rule can be justified in more than one way; in Section~\ref{sec:D} below we argue in terms of what it means to say that certain information is ``contained'' in a probability distribution.
Let us ask now whether the constraint rule \eqref{B1} is consistent with, or derivable from, the usual principles of Bayesian inference.

If we decide to use maximum entropy based on expectations of certain specified functions $\{f_1(x)\ldots f_1(x)\}$, then we know in advance that our final distribution will have the mathematical form

\begin{equation}
	\label{B72}
	p(x_i\given H) = \frac{1}{Z(\lambda_1\ldots\lambda_m)} \exp\left[-\lambda_1f_1(x_i) - \cdots - \lambda_mf_m(x_i)\right]
\end{equation}
and nothing prevents us from thinking of this as defining a class of sampling distributions parameterized by the Lagrange multipliers $\lambda_k$, the parameter space consisting of all values of $\{\lambda_1\ldots \lambda_m\}$ which lead to normalizable distributions \eqref{B72}.
Choosing a specific distribution from this class is then equivalent to making an estimate of the parameters $\lambda_k$.
But parameter estimation is a standard problem of statistical inference.

The class $C$ of hypothesis being considered is thus specified; any particular choice of the $\{\lambda_1\ldots \lambda_m\}$ may be regarded as defining a particular hypothesis $H\epsilon C$.
However, the class $C$ does not determine any particular choice of the functions $\{f_1(x) \ldots f_m(x)\}$.
For, if A is any non-singular ($m\times m$) matrix, we can carry out a linear transformation
\begin{equation}
	\label{B73}
	\sum_{k=1}^{m}\lambda_k \cdot f_k(x) = \sum_{j=1}^{m} \lambda^{*}_j\cdot f^{*}_j(x)
\end{equation}
where
\begin{align*}
	\refstepcounter{equation}
	\lambda^{*}_j \equiv \sum_{k} \lambda_k \cdot A_{kj}\tag{\theequation a}\label{B74a}\\
	f^{*}_j \equiv \sum_{k} (A^{-1})_{kj} \cdot f_k \tag{\theequation b}\label{B74b}
\end{align*}
and the class of distributions \eqref{B72} can be written equally well as
\begin{equation}
	\label{B75}
	p(x_i\given H) = \frac{1}{Z(\lambda^{*}_1\ldots\lambda^{*}_m)} \exp\left[-\lambda^{*}_1f^{*}_1(x_i) - \cdots - \lambda^{*}_mf^{*}_m(x_i)\right]
\end{equation}
As the $\{\lambda^{*}_1\ldots \lambda^{*}_m\}$ vary over their range, we generate exactly the same family of probability distributions as \eqref{B72}.
The class $C$ is therefore characteristic, not of any particular choice of the $\{f_1(x) \ldots f_m(x)\}$, but of the \emph{linear manifold} $M(C)$ spanned by them.

If the $f_k(x)$ are linearly independent, the manifold M(C) has dimensionality $m$.
Otherwise, $M(C)$ is of some lower dimensionality $m'<m$;
the set of functions $\{f_1(x) \ldots f_m(x)\}$ is then redundant, in the sense that at least one of them could be removed without changing the class $C$.
While the presence of redundant functions $f_k(x)$ proves to be harmless in that it does not affect the actual results of entropy maximization (Jaynes, \cite{jaynes68}{1968}), it is a nuisance for present purposes [Eq.~\eqref{B81} below].
In the following we assume that any redundant functions have been removed, so that $m' = m$.

Suppose now that $x_i$ is the result of some random experiment that has been repeated $r$ times, and we have obtained the data
\begin{equation}
	D \equiv \{x_1 \text{ true } r_1 \text{ times},\; x_2 \text{ true } r_2 \text{ times}, \ldots, x_n \text{ true } r_n \text{ times}\}
\end{equation}
Of course, $\sum r_i = r$.
Out of all hypotheses $H\epsilon C$, which is most strongly supported by the data $D$ according to the Bayesian, or likelihood, criterion?
To answer this, choose any particular hypothesis $H_0 \equiv \{\lambda^{(0)}_1\ldots \lambda^{(0)}_m\}$ as the ``null hypothesis'' and test it against any other hypothesis $H \equiv \{\lambda_1\ldots \lambda_m\}$ in $C$ by Bayes’ theorem.
The log-likelihood ratio in favor of $H$ over $H_0$ is
\begin{equation}
	\label{B77}
\begin{split}
	L
	& \equiv \log \frac{P(D\given H)}{P(D\given H_0)} = \sum_{i=1}^{n} r_1 \log \left[p_i/p^{(0)}_i\right]\\
	& r\left[\log(Z_0/Z) + \sum_{k=1}^{m} \left( \lambda^{(0)}_k - \lambda^{(0)}_k \right)\overline{f}_k \right]
\end{split}
\end{equation}
where
\begin{equation}
	\label{B78}
	\overline{f}_k = \frac{1}{r} \sum_{i=1}^{n} r_i f_k(x_i)
\end{equation}
is the \emph{measured} average of $f_k(x)$, as found in the experiment.
Out of all hypotheses in class $C$ the one most strongly supported by the data $D$ is the one for which the first variation vanishes:
\begin{equation}
	\label{B79}
	\ddel{L} = -r \sum_{k=1}^{m} \left[\pdv{}{\lambda_k} \log Z + \overline{f}_k\right] \ddel{\lambda_k} = 0
\end{equation}
But from \eqref{B4}, this just yields our constraint rule \eqref{B1}
\begin{equation}
	\label{B80}
	\mean{f_k} = \overline{f}_k,\qquad 1\leq k\leq m
\end{equation}
To show that this yields a true maximum, form the second variation and note that the covariance matrix
\begin{equation}
	\label{B81}
	\pdv{\log Z}{\lambda_j, \lambda_k} = \mean{f_j f_k} - \mean{f_j} \mean{f_k}
\end{equation}
is positive definite almost everywhere on the parameter space if the $f_k(x)$ are linearly independent.

Evidently, this result is invariant under the aforementioned linear transformations \eqref{B74a} and \eqref{B74b}; i.e., we shall be led to the same final distribution satisfying \eqref{B80} however the $f_k(x)$ are defined.

Therefore, we can state our conclusion as follows:

\begin{equation}
	\label{B82}
	\parbox{0.8\linewidth}{%
		Out of all hypotheses in class $C$, the data $D$ support most strongly that one for which the expectation $\mean{f(x)}$ is equal to the measured average $\overline{f}(x)$ for every function $f(x)$ in the linear manifold $M(C)$.}
\end{equation}
This appears to the writer as a rather complete answer to some objections that have been raised to the constraint rule.
We are not, after all, confusing two averages; it is a derivable consequence of probability theory that we should set them equal.
Maximizing the entropy subject to the constraints \eqref{B80}, is equivalent to (i.e., it leads to the same result as) maximizing the likelihood over the manifold of sampling distributions picked out by maximum entropy.

\subsection{Forney's Question}
An interesting question related to this was put to me by G.~David Forney in 1963.
The procedure \eqref{B1} uses only the \emph{numerical value} of $F$, and it seems to make no difference whether this was a measured average over $20$ observations, or $20\,000$.
Yet there is surely a difference in our state of knowledge--our degree of confidence in the accuracy of $F$--that depends on $N$.
The maximum-entropy method seems to ignore this.
Shouldn't our final distribution depend on $N$ as well as $F$?

It is better to answer a question 15 years late than not at all.
We can do this on both the philosophical and the technical level.
Philosophically, we are back to the question: ``What is the specific problem being solved?''

In the problem I am considering $F$ is simply a number given to us in the statement of the problem.
Within the context of that problem, $F$ is exact by definition and it makes no difference how it was obtained.
It might, for example, be only the guess of an idiot, and not obtained from any measurement at all.
Nevertheless, that is the number given to us, and our job is not to question it, but to do the best we can with it.

This may seem like an inflexible, cavalier attitude; I am convinced that nothing short of it can ever remove the ambiguity of ``What is the problem?'' that has plagued probability theory for two centuries.
Just as Rowlinson was impelled to invent an Urn Model that was not specified in the statement of the problem, you and I might, in some cases, feel the urge to put more structure into this problem than I have used.
Indeed, we demand the right to do this.
But then, let us recognize that we are considering a \emph{different} problem than pure ``classical'' maximum entropy; and it becomes a technical question, not a philosophical one, whether with some new model structure we shall get different results.
Clearly, the answer must be sometimes yes, sometimes no, depending on the specific model structure assumed.
But it turns out that the answer is ``no'' far more often than one might have expected.

Perhaps the first thought that comes to one's mind is that any uncertainty as to the value of $F$ ought to be allowed for by averaging the maximum-entropy distribution $p_i(F)$ over the possible values of $F$.
But the maximum-entropy distribution is, by construction, already as ``uncertain'' as it can get for the stated mean value.
Any averaging can only result in a distribution with still higher entropy, which will therefore necessarily violate the mean value number given to us.
This hardly seems to take us in the direction wanted; i.e., we are already up against the wall from having maximized the entropy in the first place.

But such averaging was only an \emph{ad hoc} suggestion; and in fact the Principle of Maximum Entropy already provides the proper means by which any testable information can be built into our probability assignments.
If we wish only to incorporate information about the accuracy with which f is known, no new model structure is needed; the way to do this is to impose another constraint.
In addition to $\mean{f}$ we may specify $\mean{f^2}$; or indeed, any number of moments $\mean{f^n}$ or more general functions $\mean{h(f)}$.
Each such constraint will be accompanied by its Lagrange multiplier $\lambda_1$ and the general maximum-entropy formalism already allows for this.

Of course, whenever information of this kind is available it should in principle be taken into account in this way.
I would ``hold it to be self-evident'' that for any problem of inference, the ideal toward which we should aim is that all the relevant information we have ought to be incorporated explicitly into our equations; while at the same time, ``objectivity'' requires that we carefully avoid assuming any information that we do not possess.
The Principle of Maximum Entropy, like Ockham, tells us to refrain from inventing Urn Models when we have no Urn.

But in practice, some kinds of information prove to be far more relevant than others, and this extra information about the accuracy of $F$ usually affects our actual conclusions so little that it is hardly worth the effort.
This is particularly true in statistical mechanics, due to the enormously high dimensionality of the phase space.
Here the effect of specifying any reasonable accuracy in $F$ is usually completely negligible.
However, there are occasional exceptions; and whenever this extra information does make an appreciable difference it would, of course, be wrong to ignore it.