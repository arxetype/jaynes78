\section*{Summary}
In Part~\ref{sec:A} we place the Principle of Maximum Entropy in its historical perspective as a natural extension and unification of two separate lines of development, both of which had long used special cases of it.
The first line is identified with the names Bernoulli, Laplace, Jeffreys, Cox; the second with Maxwell, Boltzmann, Gibbs, Shannon.

Part~\ref{sec:B} considers some general properties of the present maximum entropy, formalism, stressing its consistency and inter-derivability with the other principles of probability theory.
In this connection we answer some published criticisms of the principle.

In part~\ref{sec:C} we try to view the principle in the wider context of Statistical Decision Theory in general, and speculate on possible future applications and further theoretical developments.
The Principle of Maximum Entropy, together with the seemingly disparate principles of Group Invariance and Marginalization, may in time be seen as special cases of a still more general principle for translating information into a probability assignment.

Part~\ref{sec:D}, which should logically precede C, is relegated to the end because it is of a more technical nature, requiring also the full formalism of quantum mechanics.
Readers not familiar with this will find the first three Sections a self-contained exposition.

In Part~\ref{sec:D} we present some of the details and results of what is at present the most highly developed application of the Principle of Maximum Entropy; the extension of Gibbs's formalism to irreversible processes.
Here we consider the most general application of the principle, without taking advantage of any	special features (such as interest in only a subspace of states, or a subset of operators) that might be found in particular problems.
An alternative formulation, which does take such advantage---and is thus closer to the spirit of previous ``kinetic equation'' approaches at the cost of some generality, appears in the presentation of Dr. Baldwin Robertson.